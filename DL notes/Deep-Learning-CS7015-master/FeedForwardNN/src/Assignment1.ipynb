{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 1234\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "import fashion_mnist_loader as loader\n",
    "import FeedForwardNetwork as nn\n",
    "import helper\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 12,12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Data Loaded Successfully...\n",
      "Loading Data...\n",
      "Data Loaded Successfully...\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train, train_mean, train_std) = loader.load_data('../data/train.csv', data = 'train', normalise = True )\n",
    "(X_val, Y_val, train_mean, train_std) = loader.load_data('../data/val.csv', data='val', normalise = True, mean = train_mean, std = train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Kaggle Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Kaggle Accuracy = 83.43 %\n",
    "    - Final updated parameters are not stored as I submitted it earlier (sorry for that)\n",
    "    - but to get the parameters, just run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "loss_function = 'ce'\n",
    "seed = 1234\n",
    "# len(g) = len(size) - 1\n",
    "g = ['tanh', 'tanh', 'stable_softmax']\n",
    "\n",
    "#for i in range(0, len(hidden_neurons)):\n",
    "neurons = 100\n",
    "size = [784, neurons, neurons, n_classes]\n",
    "\n",
    "network = nn.FeedForwardNetwork(size, g, loss_function, seed)\n",
    "\n",
    "expt_dir = '../logs/kaggle/'\n",
    "\n",
    "filepath = expt_dir + '4/'\n",
    "helper.make_sure_path_exists(filepath)\n",
    "\n",
    "network.saveNeuralNetwork(filepath)\n",
    "\n",
    "max_epochs=70\n",
    "eta = 0.07\n",
    "gamma = 0.9\n",
    "batch_size = X_train.shape[0]\n",
    "\n",
    "(predictions, training_loss, validation_loss) = network.trainingAlgo( X_train, Y_train, X_val, Y_val, filepath, opt = 'momentum', momentum = gamma,eta=eta, anneal = True, batch_size = batch_size, max_epochs = max_epochs)\n",
    "(id, X_test) = loader.load_test_data('../data/test.csv', normalise = True, mean = train_mean, std = train_std)\n",
    "\n",
    "# Making predictions on test data\n",
    "Y_test = network.forward_pass(X_test)\n",
    "Y_test = Y_test.T\n",
    "testPredictions = []\n",
    "for i in range(0, Y_test.shape[0]):\n",
    "    testPredictions.append(np.argmax(Y_test[i]))\n",
    "submit = pd.DataFrame(columns = ['id','label'])\n",
    "submit['id'] = id\n",
    "submit['label'] = testPredictions\n",
    "results = submit.sort_values(by=['id'], ascending=[True])\n",
    "results.to_csv(filepath + 'results.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Kaggle Accuracy = 84.13 % \n",
    "# Best Model\n",
    "    - parameters are stored in '../logs/kaggle/1/parameters/' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss 12256.113893812771\n",
      "Epoch 0 Validation Loss 3333.294636433219\n",
      "Epoch 0 : Training Accuracy : 83.76%\n",
      "Epoch 0 : Validation Accuracy : 83.8%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 1 Training Loss 11692.806980879306\n",
      "Epoch 1 Validation Loss 3320.0662009516745\n",
      "Epoch 1 : Training Accuracy : 85.13%\n",
      "Epoch 1 : Validation Accuracy : 84.66%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 2 Training Loss 11564.287115050915\n",
      "Epoch 2 Validation Loss 3314.050946518312\n",
      "Epoch 2 : Training Accuracy : 85.77%\n",
      "Epoch 2 : Validation Accuracy : 85.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 3 Training Loss 11487.161890394564\n",
      "Epoch 3 Validation Loss 3310.6957699765553\n",
      "Epoch 3 : Training Accuracy : 86.15%\n",
      "Epoch 3 : Validation Accuracy : 85.38%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 4 Training Loss 11432.200765968555\n",
      "Epoch 4 Validation Loss 3308.5898036959543\n",
      "Epoch 4 : Training Accuracy : 86.43%\n",
      "Epoch 4 : Validation Accuracy : 85.62%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 5 Training Loss 11390.206869045396\n",
      "Epoch 5 Validation Loss 3307.281138144078\n",
      "Epoch 5 : Training Accuracy : 86.61%\n",
      "Epoch 5 : Validation Accuracy : 85.74%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 6 Training Loss 11357.061153570581\n",
      "Epoch 6 Validation Loss 3306.4209660778915\n",
      "Epoch 6 : Training Accuracy : 86.78%\n",
      "Epoch 6 : Validation Accuracy : 85.84%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 7 Training Loss 11330.126411743395\n",
      "Epoch 7 Validation Loss 3305.7702746727473\n",
      "Epoch 7 : Training Accuracy : 86.92%\n",
      "Epoch 7 : Validation Accuracy : 85.86%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 8 Training Loss 11307.727429934988\n",
      "Epoch 8 Validation Loss 3305.200044670368\n",
      "Epoch 8 : Training Accuracy : 87.06%\n",
      "Epoch 8 : Validation Accuracy : 85.94%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 9 Training Loss 11288.756335285281\n",
      "Epoch 9 Validation Loss 3304.7205055674663\n",
      "Epoch 9 : Training Accuracy : 87.21%\n",
      "Epoch 9 : Validation Accuracy : 85.9%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 10 Training Loss 11272.611947700196\n",
      "Epoch 10 Validation Loss 3304.39773487469\n",
      "Epoch 10 : Training Accuracy : 87.31%\n",
      "Epoch 10 : Validation Accuracy : 85.92%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 11 Training Loss 11258.91844012741\n",
      "Epoch 11 Validation Loss 3304.2385201550833\n",
      "Epoch 11 : Training Accuracy : 87.38%\n",
      "Epoch 11 : Validation Accuracy : 85.94%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 12 Training Loss 11247.26982836824\n",
      "Epoch 12 Validation Loss 3304.1921518027857\n",
      "Epoch 12 : Training Accuracy : 87.43%\n",
      "Epoch 12 : Validation Accuracy : 86.1%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 13 Training Loss 11237.292613369527\n",
      "Epoch 13 Validation Loss 3304.2025140309656\n",
      "Epoch 13 : Training Accuracy : 87.53%\n",
      "Epoch 13 : Validation Accuracy : 86.04%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 14 Training Loss 11228.694849768495\n",
      "Epoch 14 Validation Loss 3304.2528974613087\n",
      "Epoch 14 : Training Accuracy : 87.63%\n",
      "Epoch 14 : Validation Accuracy : 85.94%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 15 Training Loss 11221.23032771655\n",
      "Epoch 15 Validation Loss 3304.33435687909\n",
      "Epoch 15 : Training Accuracy : 87.65%\n",
      "Epoch 15 : Validation Accuracy : 86.0%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 16 Training Loss 11214.774319229558\n",
      "Epoch 16 Validation Loss 3304.424950144199\n",
      "Epoch 16 : Training Accuracy : 87.67%\n",
      "Epoch 16 : Validation Accuracy : 86.14%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 17 Training Loss 11209.337203747018\n",
      "Epoch 17 Validation Loss 3304.5120040340216\n",
      "Epoch 17 : Training Accuracy : 87.72%\n",
      "Epoch 17 : Validation Accuracy : 86.22%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 18 Training Loss 11204.868252920625\n",
      "Epoch 18 Validation Loss 3304.624503683379\n",
      "Epoch 18 : Training Accuracy : 87.8%\n",
      "Epoch 18 : Validation Accuracy : 86.32%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 19 Training Loss 11201.152950392654\n",
      "Epoch 19 Validation Loss 3304.806957006235\n",
      "Epoch 19 : Training Accuracy : 87.81%\n",
      "Epoch 19 : Validation Accuracy : 86.34%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 20 Training Loss 11197.906852499238\n",
      "Epoch 20 Validation Loss 3305.093417374217\n",
      "Epoch 20 : Training Accuracy : 87.85%\n",
      "Epoch 20 : Validation Accuracy : 86.36%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 21 Training Loss 11194.876617980884\n",
      "Epoch 21 Validation Loss 3305.4994260240296\n",
      "Epoch 21 : Training Accuracy : 87.84%\n",
      "Epoch 21 : Validation Accuracy : 86.18%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 22 Training Loss 11191.943806404388\n",
      "Epoch 22 Validation Loss 3306.0000713495247\n",
      "Epoch 22 : Training Accuracy : 87.83%\n",
      "Epoch 22 : Validation Accuracy : 86.0%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 23 Training Loss 11189.072877728506\n",
      "Epoch 23 Validation Loss 3306.555651916923\n",
      "Epoch 23 : Training Accuracy : 87.86%\n",
      "Epoch 23 : Validation Accuracy : 86.02%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 24 Training Loss 11186.19960763204\n",
      "Epoch 24 Validation Loss 3307.1105891827056\n",
      "Epoch 24 : Training Accuracy : 87.85%\n",
      "Epoch 24 : Validation Accuracy : 86.06%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 25 Training Loss 11183.234553197313\n",
      "Epoch 25 Validation Loss 3307.5928457872697\n",
      "Epoch 25 : Training Accuracy : 87.83%\n",
      "Epoch 25 : Validation Accuracy : 85.86%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 26 Training Loss 11180.024095364268\n",
      "Epoch 26 Validation Loss 3307.9779829790073\n",
      "Epoch 26 : Training Accuracy : 87.83%\n",
      "Epoch 26 : Validation Accuracy : 85.9%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 27 Training Loss 11176.436230577441\n",
      "Epoch 27 Validation Loss 3308.2819098625514\n",
      "Epoch 27 : Training Accuracy : 87.86%\n",
      "Epoch 27 : Validation Accuracy : 85.94%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 28 Training Loss 11172.528358601176\n",
      "Epoch 28 Validation Loss 3308.4885939170736\n",
      "Epoch 28 : Training Accuracy : 87.92%\n",
      "Epoch 28 : Validation Accuracy : 86.02%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 29 Training Loss 11168.532000712772\n",
      "Epoch 29 Validation Loss 3308.6071436818847\n",
      "Epoch 29 : Training Accuracy : 87.9%\n",
      "Epoch 29 : Validation Accuracy : 86.16%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 30 Training Loss 11164.68980038518\n",
      "Epoch 30 Validation Loss 3308.6734605452575\n",
      "Epoch 30 : Training Accuracy : 87.87%\n",
      "Epoch 30 : Validation Accuracy : 86.14%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 31 Training Loss 11161.171921138917\n",
      "Epoch 31 Validation Loss 3308.6661158003712\n",
      "Epoch 31 : Training Accuracy : 87.87%\n",
      "Epoch 31 : Validation Accuracy : 86.2%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Training Loss 11157.989947039392\n",
      "Epoch 32 Validation Loss 3308.6121203233515\n",
      "Epoch 32 : Training Accuracy : 87.91%\n",
      "Epoch 32 : Validation Accuracy : 86.18%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 33 Training Loss 11155.122074384155\n",
      "Epoch 33 Validation Loss 3308.5530011316077\n",
      "Epoch 33 : Training Accuracy : 87.94%\n",
      "Epoch 33 : Validation Accuracy : 86.1%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 34 Training Loss 11152.533564038968\n",
      "Epoch 34 Validation Loss 3308.493993517049\n",
      "Epoch 34 : Training Accuracy : 87.97%\n",
      "Epoch 34 : Validation Accuracy : 86.1%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 35 Training Loss 11150.20090477207\n",
      "Epoch 35 Validation Loss 3308.4177218173186\n",
      "Epoch 35 : Training Accuracy : 87.99%\n",
      "Epoch 35 : Validation Accuracy : 86.24%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 36 Training Loss 11148.10131749532\n",
      "Epoch 36 Validation Loss 3308.32127291927\n",
      "Epoch 36 : Training Accuracy : 87.99%\n",
      "Epoch 36 : Validation Accuracy : 86.28%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 37 Training Loss 11146.221748394057\n",
      "Epoch 37 Validation Loss 3308.2197582803674\n",
      "Epoch 37 : Training Accuracy : 88.01%\n",
      "Epoch 37 : Validation Accuracy : 86.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 38 Training Loss 11144.552285212816\n",
      "Epoch 38 Validation Loss 3308.1081699933384\n",
      "Epoch 38 : Training Accuracy : 87.99%\n",
      "Epoch 38 : Validation Accuracy : 86.36%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 39 Training Loss 11143.032520029537\n",
      "Epoch 39 Validation Loss 3307.981421529609\n",
      "Epoch 39 : Training Accuracy : 88.01%\n",
      "Epoch 39 : Validation Accuracy : 86.24%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 40 Training Loss 11141.643887725675\n",
      "Epoch 40 Validation Loss 3307.8404148870163\n",
      "Epoch 40 : Training Accuracy : 88.08%\n",
      "Epoch 40 : Validation Accuracy : 86.36%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 41 Training Loss 11140.426115637403\n",
      "Epoch 41 Validation Loss 3307.6824131371104\n",
      "Epoch 41 : Training Accuracy : 88.14%\n",
      "Epoch 41 : Validation Accuracy : 86.36%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 42 Training Loss 11139.402948850253\n",
      "Epoch 42 Validation Loss 3307.520428816042\n",
      "Epoch 42 : Training Accuracy : 88.13%\n",
      "Epoch 42 : Validation Accuracy : 86.3%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 43 Training Loss 11138.54950587768\n",
      "Epoch 43 Validation Loss 3307.37932689319\n",
      "Epoch 43 : Training Accuracy : 88.15%\n",
      "Epoch 43 : Validation Accuracy : 86.24%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 44 Training Loss 11137.859662299234\n",
      "Epoch 44 Validation Loss 3307.2587352981573\n",
      "Epoch 44 : Training Accuracy : 88.16%\n",
      "Epoch 44 : Validation Accuracy : 86.46%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 45 Training Loss 11137.34462837269\n",
      "Epoch 45 Validation Loss 3307.1515060790107\n",
      "Epoch 45 : Training Accuracy : 88.19%\n",
      "Epoch 45 : Validation Accuracy : 86.46%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 46 Training Loss 11136.999767818204\n",
      "Epoch 46 Validation Loss 3307.064839998431\n",
      "Epoch 46 : Training Accuracy : 88.18%\n",
      "Epoch 46 : Validation Accuracy : 86.5%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 47 Training Loss 11136.787378464614\n",
      "Epoch 47 Validation Loss 3307.012288578559\n",
      "Epoch 47 : Training Accuracy : 88.17%\n",
      "Epoch 47 : Validation Accuracy : 86.42%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 48 Training Loss 11136.66528414822\n",
      "Epoch 48 Validation Loss 3306.990356835624\n",
      "Epoch 48 : Training Accuracy : 88.16%\n",
      "Epoch 48 : Validation Accuracy : 86.36%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 49 Training Loss 11136.564727315888\n",
      "Epoch 49 Validation Loss 3306.9935886678677\n",
      "Epoch 49 : Training Accuracy : 88.15%\n",
      "Epoch 49 : Validation Accuracy : 86.52%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 50 Training Loss 11136.388697837912\n",
      "Epoch 50 Validation Loss 3307.027644168394\n",
      "Epoch 50 : Training Accuracy : 88.17%\n",
      "Epoch 50 : Validation Accuracy : 86.62%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 51 Training Loss 11136.047144234097\n",
      "Epoch 51 Validation Loss 3307.100934253387\n",
      "Epoch 51 : Training Accuracy : 88.14%\n",
      "Epoch 51 : Validation Accuracy : 86.66%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 52 Training Loss 11135.50701532132\n",
      "Epoch 52 Validation Loss 3307.2072839061016\n",
      "Epoch 52 : Training Accuracy : 88.13%\n",
      "Epoch 52 : Validation Accuracy : 86.52%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 53 Training Loss 11134.772973531359\n",
      "Epoch 53 Validation Loss 3307.323575904234\n",
      "Epoch 53 : Training Accuracy : 88.1%\n",
      "Epoch 53 : Validation Accuracy : 86.44%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 54 Training Loss 11133.847054515803\n",
      "Epoch 54 Validation Loss 3307.4241740731422\n",
      "Epoch 54 : Training Accuracy : 88.1%\n",
      "Epoch 54 : Validation Accuracy : 86.48%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 55 Training Loss 11132.71263940271\n",
      "Epoch 55 Validation Loss 3307.4937532042677\n",
      "Epoch 55 : Training Accuracy : 88.09%\n",
      "Epoch 55 : Validation Accuracy : 86.54%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 56 Training Loss 11131.357938769648\n",
      "Epoch 56 Validation Loss 3307.529817912541\n",
      "Epoch 56 : Training Accuracy : 88.11%\n",
      "Epoch 56 : Validation Accuracy : 86.4%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 57 Training Loss 11129.814596983178\n",
      "Epoch 57 Validation Loss 3307.5421455980977\n",
      "Epoch 57 : Training Accuracy : 88.13%\n",
      "Epoch 57 : Validation Accuracy : 86.28%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 58 Training Loss 11128.142668111972\n",
      "Epoch 58 Validation Loss 3307.5503136038287\n",
      "Epoch 58 : Training Accuracy : 88.16%\n",
      "Epoch 58 : Validation Accuracy : 86.34%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 59 Training Loss 11126.416174173095\n",
      "Epoch 59 Validation Loss 3307.571783478389\n",
      "Epoch 59 : Training Accuracy : 88.15%\n",
      "Epoch 59 : Validation Accuracy : 86.2%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 60 Training Loss 11124.721041421695\n",
      "Epoch 60 Validation Loss 3307.616211417804\n",
      "Epoch 60 : Training Accuracy : 88.17%\n",
      "Epoch 60 : Validation Accuracy : 86.28%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 61 Training Loss 11123.124482330857\n",
      "Epoch 61 Validation Loss 3307.681800761031\n",
      "Epoch 61 : Training Accuracy : 88.17%\n",
      "Epoch 61 : Validation Accuracy : 86.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 62 Training Loss 11121.669376045382\n",
      "Epoch 62 Validation Loss 3307.7538335872114\n",
      "Epoch 62 : Training Accuracy : 88.17%\n",
      "Epoch 62 : Validation Accuracy : 86.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 63 Training Loss 11120.3790324287\n",
      "Epoch 63 Validation Loss 3307.817878438247\n",
      "Epoch 63 : Training Accuracy : 88.14%\n",
      "Epoch 63 : Validation Accuracy : 86.28%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Training Loss 11119.276161153803\n",
      "Epoch 64 Validation Loss 3307.8735836563274\n",
      "Epoch 64 : Training Accuracy : 88.12%\n",
      "Epoch 64 : Validation Accuracy : 86.36%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 65 Training Loss 11118.388862791793\n",
      "Epoch 65 Validation Loss 3307.9308186388757\n",
      "Epoch 65 : Training Accuracy : 88.14%\n",
      "Epoch 65 : Validation Accuracy : 86.28%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 66 Training Loss 11117.73817962539\n",
      "Epoch 66 Validation Loss 3307.994917984242\n",
      "Epoch 66 : Training Accuracy : 88.11%\n",
      "Epoch 66 : Validation Accuracy : 86.28%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 67 Training Loss 11117.323976761005\n",
      "Epoch 67 Validation Loss 3308.0630784642913\n",
      "Epoch 67 : Training Accuracy : 88.11%\n",
      "Epoch 67 : Validation Accuracy : 86.42%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 68 Training Loss 11117.12350956932\n",
      "Epoch 68 Validation Loss 3308.132205165468\n",
      "Epoch 68 : Training Accuracy : 88.09%\n",
      "Epoch 68 : Validation Accuracy : 86.3%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 69 Training Loss 11117.11265447772\n",
      "Epoch 69 Validation Loss 3308.202482119205\n",
      "Epoch 69 : Training Accuracy : 88.08%\n",
      "Epoch 69 : Validation Accuracy : 86.2%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 70 Training Loss 11117.270900747872\n",
      "Epoch 70 Validation Loss 3308.2694542051177\n",
      "Epoch 70 : Training Accuracy : 88.04%\n",
      "Epoch 70 : Validation Accuracy : 86.2%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 71 Training Loss 11117.56915665974\n",
      "Epoch 71 Validation Loss 3308.3209835733724\n",
      "Epoch 71 : Training Accuracy : 88.01%\n",
      "Epoch 71 : Validation Accuracy : 86.12%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 72 Training Loss 11117.972100224593\n",
      "Epoch 72 Validation Loss 3308.345708215431\n",
      "Epoch 72 : Training Accuracy : 87.99%\n",
      "Epoch 72 : Validation Accuracy : 86.24%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 73 Training Loss 11118.43230240232\n",
      "Epoch 73 Validation Loss 3308.342287628491\n",
      "Epoch 73 : Training Accuracy : 87.99%\n",
      "Epoch 73 : Validation Accuracy : 86.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 74 Training Loss 11118.913927937614\n",
      "Epoch 74 Validation Loss 3308.3201787205726\n",
      "Epoch 74 : Training Accuracy : 87.99%\n",
      "Epoch 74 : Validation Accuracy : 86.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 75 Training Loss 11119.413356031919\n",
      "Epoch 75 Validation Loss 3308.288514067487\n",
      "Epoch 75 : Training Accuracy : 87.98%\n",
      "Epoch 75 : Validation Accuracy : 86.4%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 76 Training Loss 11119.940499071732\n",
      "Epoch 76 Validation Loss 3308.251443325217\n",
      "Epoch 76 : Training Accuracy : 87.98%\n",
      "Epoch 76 : Validation Accuracy : 86.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 77 Training Loss 11120.50231304351\n",
      "Epoch 77 Validation Loss 3308.211118769039\n",
      "Epoch 77 : Training Accuracy : 87.98%\n",
      "Epoch 77 : Validation Accuracy : 86.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 78 Training Loss 11121.135672367453\n",
      "Epoch 78 Validation Loss 3308.1638754915934\n",
      "Epoch 78 : Training Accuracy : 87.98%\n",
      "Epoch 78 : Validation Accuracy : 86.4%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 79 Training Loss 11121.853443623479\n",
      "Epoch 79 Validation Loss 3308.1046771027213\n",
      "Epoch 79 : Training Accuracy : 87.96%\n",
      "Epoch 79 : Validation Accuracy : 86.32%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 80 Training Loss 11122.62870087228\n",
      "Epoch 80 Validation Loss 3308.0327938876035\n",
      "Epoch 80 : Training Accuracy : 87.92%\n",
      "Epoch 80 : Validation Accuracy : 86.3%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 81 Training Loss 11123.435852128807\n",
      "Epoch 81 Validation Loss 3307.951707910307\n",
      "Epoch 81 : Training Accuracy : 87.93%\n",
      "Epoch 81 : Validation Accuracy : 86.32%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 82 Training Loss 11124.24479234187\n",
      "Epoch 82 Validation Loss 3307.8703158684048\n",
      "Epoch 82 : Training Accuracy : 87.91%\n",
      "Epoch 82 : Validation Accuracy : 86.36%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 83 Training Loss 11125.035638994375\n",
      "Epoch 83 Validation Loss 3307.8038028019155\n",
      "Epoch 83 : Training Accuracy : 87.93%\n",
      "Epoch 83 : Validation Accuracy : 86.32%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 84 Training Loss 11125.807306304558\n",
      "Epoch 84 Validation Loss 3307.7599761766323\n",
      "Epoch 84 : Training Accuracy : 87.9%\n",
      "Epoch 84 : Validation Accuracy : 86.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 85 Training Loss 11126.537738214336\n",
      "Epoch 85 Validation Loss 3307.7283595740637\n",
      "Epoch 85 : Training Accuracy : 87.87%\n",
      "Epoch 85 : Validation Accuracy : 86.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 86 Training Loss 11127.21365814123\n",
      "Epoch 86 Validation Loss 3307.7013778152204\n",
      "Epoch 86 : Training Accuracy : 87.88%\n",
      "Epoch 86 : Validation Accuracy : 86.18%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 87 Training Loss 11127.859843842323\n",
      "Epoch 87 Validation Loss 3307.693790855968\n",
      "Epoch 87 : Training Accuracy : 87.89%\n",
      "Epoch 87 : Validation Accuracy : 86.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 88 Training Loss 11128.516321216199\n",
      "Epoch 88 Validation Loss 3307.6859201467632\n",
      "Epoch 88 : Training Accuracy : 87.9%\n",
      "Epoch 88 : Validation Accuracy : 86.32%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 89 Training Loss 11129.177973838612\n",
      "Epoch 89 Validation Loss 3307.667216867884\n",
      "Epoch 89 : Training Accuracy : 87.92%\n",
      "Epoch 89 : Validation Accuracy : 86.34%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 90 Training Loss 11129.814329048128\n",
      "Epoch 90 Validation Loss 3307.6680641416506\n",
      "Epoch 90 : Training Accuracy : 87.91%\n",
      "Epoch 90 : Validation Accuracy : 86.32%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 91 Training Loss 11130.419548440328\n",
      "Epoch 91 Validation Loss 3307.7108103331457\n",
      "Epoch 91 : Training Accuracy : 87.91%\n",
      "Epoch 91 : Validation Accuracy : 86.26%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 92 Training Loss 11131.005563134571\n",
      "Epoch 92 Validation Loss 3307.7914668893272\n",
      "Epoch 92 : Training Accuracy : 87.9%\n",
      "Epoch 92 : Validation Accuracy : 86.22%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 93 Training Loss 11131.572889698085\n",
      "Epoch 93 Validation Loss 3307.8984610661164\n",
      "Epoch 93 : Training Accuracy : 87.89%\n",
      "Epoch 93 : Validation Accuracy : 86.22%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 94 Training Loss 11132.110635219\n",
      "Epoch 94 Validation Loss 3308.013912603713\n",
      "Epoch 94 : Training Accuracy : 87.9%\n",
      "Epoch 94 : Validation Accuracy : 86.28%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 95 Training Loss 11132.60122777978\n",
      "Epoch 95 Validation Loss 3308.1256008637088\n",
      "Epoch 95 : Training Accuracy : 87.9%\n",
      "Epoch 95 : Validation Accuracy : 86.38%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Training Loss 11133.006019791737\n",
      "Epoch 96 Validation Loss 3308.227235324329\n",
      "Epoch 96 : Training Accuracy : 87.9%\n",
      "Epoch 96 : Validation Accuracy : 86.42%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 97 Training Loss 11133.25241461827\n",
      "Epoch 97 Validation Loss 3308.3091515996225\n",
      "Epoch 97 : Training Accuracy : 87.88%\n",
      "Epoch 97 : Validation Accuracy : 86.36%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 98 Training Loss 11133.262693938554\n",
      "Epoch 98 Validation Loss 3308.3673109132337\n",
      "Epoch 98 : Training Accuracy : 87.91%\n",
      "Epoch 98 : Validation Accuracy : 86.24%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 99 Training Loss 11133.04995331288\n",
      "Epoch 99 Validation Loss 3308.4139684385373\n",
      "Epoch 99 : Training Accuracy : 87.91%\n",
      "Epoch 99 : Validation Accuracy : 86.16%\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "Loading Data...\n",
      "Data Loaded Successfully...\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "loss_function = 'ce'\n",
    "seed = 1234\n",
    "# len(g) = len(size) - 1\n",
    "g = ['tanh', 'tanh', 'tanh', 'stable_softmax']\n",
    "\n",
    "#for i in range(0, len(hidden_neurons)):\n",
    "neurons = 150\n",
    "size = [784, neurons, neurons, neurons, n_classes]\n",
    "\n",
    "network = nn.FeedForwardNetwork(size, g, loss_function, seed)\n",
    "\n",
    "expt_dir = '../logs/kaggle/'\n",
    "\n",
    "filepath = expt_dir + '5/'\n",
    "helper.make_sure_path_exists(filepath)\n",
    "\n",
    "network.saveNeuralNetwork(filepath)\n",
    "\n",
    "max_epochs=100\n",
    "eta = 0.001\n",
    "gamma = 0.9\n",
    "batch_size = 50\n",
    "\n",
    "(predictions, training_loss, validation_loss) = network.trainingAlgo( X_train, Y_train, X_val, Y_val, filepath, opt = 'momentum', momentum = gamma,eta=eta, anneal = True, batch_size = batch_size, max_epochs = max_epochs)\n",
    "(id, X_test) = loader.load_test_data('../data/test.csv', normalise = True, mean = train_mean, std = train_std)\n",
    "\n",
    "# Making predictions on test data\n",
    "Y_test = network.forward_pass(X_test)\n",
    "Y_test = Y_test.T\n",
    "testPredictions = []\n",
    "for i in range(0, Y_test.shape[0]):\n",
    "    testPredictions.append(np.argmax(Y_test[i]))\n",
    "submit = pd.DataFrame(columns = ['id','label'])\n",
    "submit['id'] = id\n",
    "submit['label'] = testPredictions\n",
    "results = submit.sort_values(by=['id'], ascending=[True])\n",
    "results.to_csv(filepath + 'results.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the architecture of the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "loss_function = 'ce'\n",
    "seed = 1234\n",
    "# len(g) = len(size) - 1\n",
    "g = ['sigmoid', 'sigmoid', 'stable_softmax']\n",
    "\n",
    "#for i in range(0, len(hidden_neurons)):\n",
    "neurons = 100\n",
    "size = [784, neurons, neurons, n_classes]\n",
    "\n",
    "network = nn.FeedForwardNetwork(size, g, loss_function, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4 : crossEntropy vs MSE\n",
    "    - first run it as it is (for ce)\n",
    "    - change 'ce' to 'sq' everywhere in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expt_dir = '../logs/exp4/'\n",
    "\n",
    "filepath = expt_dir + 'ce/'\n",
    "helper.make_sure_path_exists(filepath)\n",
    "\n",
    "network.saveNeuralNetwork(filepath)\n",
    "\n",
    "max_epochs=25\n",
    "eta = 0.001\n",
    "gamma = 0.9\n",
    "batch_size = X_train.shape[0]\n",
    "\n",
    "(predictions, training_loss, validation_loss) = network.trainingAlgo( X_train, Y_train, X_val, Y_val, filepath, opt = 'adam', momentum = gamma,eta=eta, anneal = False, batch_size = batch_size, max_epochs = max_epochs)\n",
    "\n",
    "(id, X_test) = loader.load_test_data('../data/test.csv', normalise = True, mean = train_mean, std = train_std)\n",
    "\n",
    "# Making predictions on test data\n",
    "Y_test = network.forward_pass(X_test)\n",
    "Y_test = Y_test.T\n",
    "testPredictions = []\n",
    "for i in range(0, Y_test.shape[0]):\n",
    "    testPredictions.append(np.argmax(Y_test[i]))\n",
    "submit = pd.DataFrame(columns = ['id','label'])\n",
    "submit['id'] = id\n",
    "submit['label'] = testPredictions\n",
    "results = submit.sort_values(by=['id'], ascending=[True])\n",
    "results.to_csv(filepath + 'results.csv', index=False, sep=',')\n",
    "\n",
    "df = pd.DataFrame(columns = ['training_loss','validation_loss'])\n",
    "df['training_loss'] = training_loss\n",
    "df['validation_loss'] = validation_loss\n",
    "df.to_csv(filepath + 'loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the graph for crossEntropy vs tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0., max_epochs, num=max_epochs)\n",
    "\n",
    "activationFunctions = ['sq', 'ce']\n",
    "\n",
    "for each in activationFunctions:\n",
    "    y_axis = pd.read_csv(expt_dir + str(each) + '/loss.csv')['training_loss']\n",
    "    y_axis= y_axis / 55000\n",
    "    if each == 'ce':\n",
    "        label = 'Cross Entropy'\n",
    "    else:\n",
    "        label = 'MSE'\n",
    "    plt.plot(x_axis, y_axis , label=label)\n",
    "    plt.scatter(x_axis, y_axis, label=None)\n",
    "    \n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.legend(prop={'size' : 16}) # prop is to set the font properties of the legend\n",
    "\n",
    "plt.title('Comparision of different Loss Functions')\n",
    "plt.show()   \n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "x_axis = np.linspace(0., max_epochs, num=max_epochs)\n",
    "\n",
    "activationFunctions = ['sq', 'ce']\n",
    "\n",
    "for each in activationFunctions:\n",
    "    y_axis = pd.read_csv(expt_dir + str(each) + '/loss.csv')['validation_loss']\n",
    "    y_axis= y_axis / 5500\n",
    "    if each == 'ce':\n",
    "        label = 'Cross Entropy'\n",
    "    else:\n",
    "        label = 'MSE'\n",
    "    plt.plot(x_axis, y_axis , label=label)\n",
    "    plt.scatter(x_axis, y_axis, label=None)\n",
    "    \n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend(prop={'size' : 16}) # prop is to set the font properties of the legend\n",
    "\n",
    "plt.title('Comparision of different Loss Functions')\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3 : Different Activation Functions\n",
    "    - first run as it is (for 'tanh')\n",
    "    - then replace 'tanh' with 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expt_dir = '../logs/exp3/'\n",
    "\n",
    "filepath = expt_dir + 'tanh/'\n",
    "helper.make_sure_path_exists(filepath)\n",
    "\n",
    "network.saveNeuralNetwork(filepath)\n",
    "\n",
    "max_epochs=25\n",
    "eta = 0.001\n",
    "gamma = 0.9\n",
    "batch_size = X_train.shape[0]\n",
    "\n",
    "(predictions, training_loss, validation_loss) = network.trainingAlgo( X_train, Y_train, X_val, Y_val, filepath, opt = 'adam', momentum = gamma,eta=eta, anneal = False, batch_size = batch_size, max_epochs = max_epochs)\n",
    "\n",
    "(id, X_test) = loader.load_test_data('../data/test.csv', normalise = True, mean = train_mean, std = train_std)\n",
    "\n",
    "# Making predictions on test data\n",
    "Y_test = network.forward_pass(X_test)\n",
    "Y_test = Y_test.T\n",
    "testPredictions = []\n",
    "for i in range(0, Y_test.shape[0]):\n",
    "    testPredictions.append(np.argmax(Y_test[i]))\n",
    "submit = pd.DataFrame(columns = ['id','label'])\n",
    "submit['id'] = id\n",
    "submit['label'] = testPredictions\n",
    "results = submit.sort_values(by=['id'], ascending=[True])\n",
    "results.to_csv(filepath + 'results.csv', index=False, sep=',')\n",
    "\n",
    "df = pd.DataFrame(columns = ['training_loss','validation_loss'])\n",
    "df['training_loss'] = training_loss\n",
    "df['validation_loss'] = validation_loss\n",
    "df.to_csv(filepath + 'loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the graph for tanh vs sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0., max_epochs, num=max_epochs)\n",
    "\n",
    "activationFunctions = ['tanh', 'sigmoid']\n",
    "\n",
    "for each in activationFunctions:\n",
    "    y_axis = pd.read_csv(expt_dir + str(each) + '/loss.csv')['training_loss']\n",
    "    y_axis= y_axis / 55000\n",
    "    plt.plot(x_axis, y_axis , label=str(each))\n",
    "    plt.scatter(x_axis, y_axis, label=None)\n",
    "    \n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.legend(prop={'size' : 16}) # prop is to set the font properties of the legend\n",
    "\n",
    "plt.title('Comparision of different Activation Functions')\n",
    "plt.show()   \n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "x_axis = np.linspace(0., max_epochs, num=max_epochs)\n",
    "\n",
    "activationFunctions = ['tanh', 'sigmoid']\n",
    "\n",
    "for each in activationFunctions:\n",
    "    y_axis = pd.read_csv(expt_dir + str(each) + '/loss.csv')['validation_loss']\n",
    "    y_axis= y_axis / 5500\n",
    "    plt.plot(x_axis, y_axis , label=str(each))\n",
    "    plt.scatter(x_axis, y_axis, label=None)\n",
    "    \n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend(prop={'size' : 16}) # prop is to set the font properties of the legend\n",
    "\n",
    "plt.title('Comparision of different Activation Functions')\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experiment 2 : Different Optimization Algorithms\n",
    "    - just change the index of optAlgo for different variant of GD\n",
    "    - tune the parameters for each optAlgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optAlgos = ['adam', 'rmsprop', 'gd', 'momentum']\n",
    "\n",
    "algo = optAlgos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expt_dir = '../logs/exp2/'\n",
    "\n",
    "filepath = expt_dir + algo + '/'\n",
    "helper.make_sure_path_exists(filepath)\n",
    "\n",
    "network.saveNeuralNetwork(filepath)\n",
    "\n",
    "max_epochs=25\n",
    "eta = 0.001\n",
    "gamma = 0.9\n",
    "batch_size = X_train.shape[0]\n",
    "\n",
    "(predictions, training_loss, validation_loss) = network.trainingAlgo( X_train, Y_train, X_val, Y_val, filepath, opt = algo, momentum = gamma,eta=eta, anneal = False, batch_size = batch_size, max_epochs = max_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(id, X_test) = loader.load_test_data('../data/test.csv', normalise = True, mean = train_mean, std = train_std)\n",
    "\n",
    "# Making predictions on test data\n",
    "Y_test = network.forward_pass(X_test)\n",
    "Y_test = Y_test.T\n",
    "testPredictions = []\n",
    "for i in range(0, Y_test.shape[0]):\n",
    "    testPredictions.append(np.argmax(Y_test[i]))\n",
    "submit = pd.DataFrame(columns = ['id','label'])\n",
    "submit['id'] = id\n",
    "submit['label'] = testPredictions\n",
    "results = submit.sort_values(by=['id'], ascending=[True])\n",
    "results.to_csv(filepath + 'results.csv', index=False, sep=',')\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns = ['training_loss', 'validation_loss'])\n",
    "df['training_loss'] = training_loss\n",
    "df['validation_loss'] = validation_loss\n",
    "\n",
    "df.to_csv(filepath + 'loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the graph for variants of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0., max_epochs, num=max_epochs)\n",
    "\n",
    "optAlgos = ['adam', 'rmsprop', 'gd', 'momentum']\n",
    "\n",
    "for each in optAlgos:\n",
    "    y_axis = pd.read_csv(expt_dir + str(each) + '/loss.csv')['training_loss']\n",
    "    y_axis= y_axis / 55000\n",
    "    plt.plot(x_axis, y_axis , label=str(each))\n",
    "    plt.scatter(x_axis, y_axis, label=None)\n",
    "    \n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.legend(prop={'size' : 16}) # prop is to set the font properties of the legend\n",
    "\n",
    "plt.title('Performance of Optimization Algos on Training Data')\n",
    "plt.show()   \n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "x_axis = np.linspace(0., max_epochs, num=max_epochs)\n",
    "\n",
    "optAlgos = ['adam', 'rmsprop', 'gd', 'momentum']\n",
    "\n",
    "for each in optAlgos:\n",
    "    y_axis = pd.read_csv(expt_dir + str(each) + '/loss.csv')['validation_loss']\n",
    "    y_axis= y_axis / 5500\n",
    "    plt.plot(x_axis, y_axis , label=str(each))\n",
    "    plt.scatter(x_axis, y_axis, label=None)\n",
    "    \n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend(prop={'size' : 16}) # prop is to set the font properties of the legend\n",
    "\n",
    "plt.title('Performance of Optimization Algos on Validation Data')\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experiment 1\n",
    "    - change singleLayer to 'twoLayer' or 'threeLayer' in filepath, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expt_dir = '../logs/exp1/'\n",
    "\n",
    "filepath = expt_dir + 'threeLayer/' + str(neurons) + '/'\n",
    "helper.make_sure_path_exists(filepath)\n",
    "\n",
    "network.saveNeuralNetwork(filepath)\n",
    "\n",
    "max_epochs=25\n",
    "eta = 0.001\n",
    "gamma = 0.7\n",
    "batch_size = X_train.shape[0]\n",
    "\n",
    "(predictions, training_loss, validation_loss) = network.trainingAlgo( X_train, Y_train, X_val, Y_val, filepath, opt = 'adam', momentum = gamma,eta=eta, anneal = False, batch_size = batch_size, max_epochs = max_epochs)\n",
    "\n",
    "(id, X_test) = loader.load_test_data('../data/test.csv', normalise = True, mean = train_mean, std = train_std)\n",
    "\n",
    "# Making predictions on test data\n",
    "Y_test = network.forward_pass(X_test)\n",
    "Y_test = Y_test.T\n",
    "testPredictions = []\n",
    "for i in range(0, Y_test.shape[0]):\n",
    "    testPredictions.append(np.argmax(Y_test[i]))\n",
    "submit = pd.DataFrame(columns = ['id','label'])\n",
    "submit['id'] = id\n",
    "submit['label'] = testPredictions\n",
    "results = submit.sort_values(by=['id'], ascending=[True])\n",
    "results.to_csv(filepath + 'results.csv', index=False, sep=',')\n",
    "\n",
    "df = pd.DataFrame(columns = ['training_loss','validation_loss'])\n",
    "df['training_loss'] = training_loss\n",
    "df['validation_loss'] = validation_loss\n",
    "df.to_csv(filepath + 'loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the graph for different number of neurons\n",
    "    - change the 'threeLayer' with 'singleLayer' or 'twoLayer' to get that layered architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_axis = np.linspace(0., max_epochs, num=max_epochs)\n",
    "\n",
    "n_neurons = ['50', '100', '200', '300']\n",
    "\n",
    "for each in n_neurons:\n",
    "    y_axis = pd.read_csv(expt_dir + 'threeLayer/' + str(each) + '/loss.csv')['training_loss']\n",
    "    y_axis= y_axis / 55000 \n",
    "    plt.plot(x_axis, y_axis , label=str(each) + ' neurons')\n",
    "    plt.scatter(x_axis, y_axis, label=None)\n",
    "    \n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend(prop={'size' : 16}) # prop is to set the font properties of the legend\n",
    "\n",
    "plt.title('Training Loss with 3 hidden layers')\n",
    "plt.show()  \n",
    "\n",
    "\n",
    "\n",
    "for each in n_neurons:\n",
    "    y_axis = pd.read_csv(expt_dir + 'threeLayer/' + str(each) + '/loss.csv')['validation_loss']\n",
    "    y_axis= y_axis / 5500 \n",
    "    plt.plot(x_axis, y_axis , label=str(each) + ' neurons')\n",
    "    plt.scatter(x_axis, y_axis, label=None)\n",
    "    \n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend(prop={'size' : 16}) # prop is to set the font properties of the legend\n",
    "\n",
    "plt.title('Validation Loss with 3 hidden layers')\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
