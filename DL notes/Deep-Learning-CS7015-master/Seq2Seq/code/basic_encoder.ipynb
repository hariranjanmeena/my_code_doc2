{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitesh/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nitesh/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from collections import Counter\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Seq2Seq Items\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell,LSTMStateTuple\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitdata(inp,outp,delimiter=' '):\n",
    "    df = pd.read_csv(inp,header = None, delimiter=delimiter)\n",
    "    df[0].to_csv(outp,index=None,header=None)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToken(file_name, token, loc='start'):\n",
    "    string_to_add = token\n",
    "    if loc=='start':\n",
    "        with open(file_name, 'r') as f:\n",
    "            file_lines = [' '.join([string_to_add, x]) for x in f.readlines()]\n",
    "    elif loc == 'end':\n",
    "        with open(file_name, 'r') as f:\n",
    "            file_lines = [' '.join([x.strip(), string_to_add, '\\n']) for x in f.readlines()]\n",
    "    else:\n",
    "        print(\"Please Enter proper location (start/end)\")\n",
    "    \n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(file_lines)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addToken('./data/WeatherGov/train/summaries.txt', token='<s>', loc='start')\n",
    "# addToken('./data/WeatherGov/train/summaries.txt', token='<unk>', loc='end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_vocab_size = 392\n",
    "# source_vocab_size = 192\n",
    "vocab_size= 392\n",
    "num_units = 512\n",
    "input_size = 256\n",
    "batch_size = 32\n",
    "source_sequence_length=122\n",
    "target_sequence_length=88\n",
    "decoder_type = 'basic' # could be basic or attention\n",
    "sentences_to_read = 25000\n",
    "tgt_sos_id = '<s>'\n",
    "tgt_eos_id = '</s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitdata('./data/table_vocabcount.txt','./data/table_vocab.txt')\n",
    "# splitdata('./data/summary_vocabcount.txt','./data/summary_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = './data/summary_vec_bk.txt'\n",
    "# df = pd.read_csv(inp,header = None, delimiter=' ')\n",
    "# x = df[df.columns[1:-1]].values\n",
    "# np.save('summary_vec', x)\n",
    "# word = np.load('./summary_vec.npy')\n",
    "# print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "\t [('</s>', 0), ('time', 1), ('min', 2), ('mean', 3), ('max', 4), ('skyCover', 5), ('mode-bucket-0-100-4', 6), ('0', 7), ('6-21', 8), ('17-30', 9)]\n",
      "\t [(0, '</s>'), (1, 'time'), (2, 'min'), (3, 'mean'), (4, 'max'), (5, 'skyCover'), (6, 'mode-bucket-0-100-4'), (7, '0'), (8, '6-21'), (9, '17-30')]\n",
      "\t Vocabulary size:  192\n",
      "Target\n",
      "\t [('</s>', 0), ('.', 1), (',', 2), ('mph', 3), ('with', 4), ('a', 5), ('of', 6), ('<s>', 7), ('wind', 8), ('<unk>', 9)]\n",
      "\t [(0, '</s>'), (1, '.'), (2, ','), (3, 'mph'), (4, 'with'), (5, 'a'), (6, 'of'), (7, '<s>'), (8, 'wind'), (9, '<unk>')]\n",
      "\t Vocabulary size:  392\n"
     ]
    }
   ],
   "source": [
    "src_dictionary = dict()\n",
    "with open('./data/table_vocab.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        #we are discarding last char as it is new line char\n",
    "        src_dictionary[line[:-1]] = len(src_dictionary)\n",
    "\n",
    "src_reverse_dictionary = dict(zip(src_dictionary.values(),src_dictionary.keys()))\n",
    "\n",
    "print('Source')\n",
    "print('\\t',list(src_dictionary.items())[:10])\n",
    "print('\\t',list(src_reverse_dictionary.items())[:10])\n",
    "print('\\t','Vocabulary size: ', len(src_dictionary))\n",
    "\n",
    "tgt_dictionary = dict()\n",
    "with open('./data/summary_vocab.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        #we are discarding last char as it is new line char\n",
    "        tgt_dictionary[line[:-1]] = len(tgt_dictionary)\n",
    "\n",
    "tgt_reverse_dictionary = dict(zip(tgt_dictionary.values(),tgt_dictionary.keys()))\n",
    "\n",
    "print('Target')\n",
    "print('\\t',list(tgt_dictionary.items())[:10])\n",
    "print('\\t',list(tgt_reverse_dictionary.items())[:10])\n",
    "print('\\t','Vocabulary size: ', len(tgt_dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tables and Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample translations (25000)\n",
      "( 0 ) Table:  temperature time 6-21 min 26 mean 43 max 53 windChill time 6-21 min 0 mean 22 max 46 windSpeed time 6-21 min 6 mean 8 max 10 mode-bucket-0-20-2 0-10 windDir time 6-21 mode SE gust time 6-21 min 0 mean 0 max 0 skyCover time 6-21 mode-bucket-0-100-4 50-75 skyCover time 6-9 mode-bucket-0-100-4 0-25 skyCover time 6-13 mode-bucket-0-100-4 25-50 skyCover time 9-21 mode-bucket-0-100-4 50-75 skyCover time 13-21 mode-bucket-0-100-4 50-75 precipPotential time 6-21 min 0 mean 2 max 12\n",
      "\n",
      "( 0 ) Summary:  Mostly sunny , with a high near 53 . Southeast wind between 7 and 9 mph . \n",
      "\n",
      "( 10000 ) Table:  temperature time 6-21 min 29 mean 31 max 35 windChill time 6-21 min 16 mean 21 max 25 windSpeed time 6-21 min 8 mean 15 max 21 mode-bucket-0-20-2 10-20 windDir time 6-21 mode WNW gust time 6-21 min 0 mean 12 max 29 skyCover time 6-21 mode-bucket-0-100-4 50-75 skyCover time 6-9 mode-bucket-0-100-4 50-75 skyCover time 6-13 mode-bucket-0-100-4 50-75 skyCover time 9-21 mode-bucket-0-100-4 50-75 skyCover time 13-21 mode-bucket-0-100-4 75-100 precipPotential time 6-21 min 15 mean 17 max 18 snowChance time 6-21 mode SChc snowChance time 6-9 mode SChc snowChance time 6-13 mode SChc snowChance time 9-21 mode SChc snowChance time 13-21 mode SChc\n",
      "\n",
      "( 10000 ) Summary:  A 20 percent chance of snow . Mostly cloudy , with a high near 35 . Breezy , with a west northwest wind 8 to 11 mph increasing to between 17 and 20 mph . Winds could gust as high as 28 mph . \n",
      "\n",
      "( 20000 ) Table:  temperature time 17-30 min 44 mean 48 max 57 windChill time 17-30 min 0 mean 29 max 49 windSpeed time 17-30 min 2 mean 2 max 3 mode-bucket-0-20-2 0-10 windDir time 17-30 mode SE gust time 17-30 min 0 mean 0 max 0 skyCover time 17-30 mode-bucket-0-100-4 50-75 skyCover time 17-21 mode-bucket-0-100-4 50-75 skyCover time 17-26 mode-bucket-0-100-4 50-75 skyCover time 21-30 mode-bucket-0-100-4 50-75 skyCover time 26-30 mode-bucket-0-100-4 50-75 precipPotential time 17-30 min 6 mean 11 max 16 rainChance time 26-30 mode SChc\n",
      "\n",
      "( 20000 ) Summary:  A slight chance of showers after 4am . Mostly cloudy , with a low around 43 . Light southeast wind . Chance of precipitation is 20 % . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading train.combined as source_sent and summaries.txt as target_sent\n",
    "source_sent = []\n",
    "target_sent = []\n",
    "\n",
    "test_source_sent = []\n",
    "test_target_sent = []\n",
    "\n",
    "\n",
    "with open('/home/nitesh/Documents/PA_4/data/WeatherGov/train/train.combined', 'r') as f:\n",
    "    for l_i, line in enumerate(f):\n",
    "#         # discarding first 20 translations as there was some\n",
    "#         # english to english translations found in the first few. which are wrong\n",
    "#         if l_i<50:\n",
    "#             continue\n",
    "        source_sent.append(line)\n",
    "        if len(source_sent)>=sentences_to_read:\n",
    "            break\n",
    "        \n",
    "            \n",
    "with open('/home/nitesh/Documents/PA_4/data/WeatherGov/train/summaries.txt', 'r') as f:\n",
    "    for l_i, line in enumerate(f):\n",
    "#         if l_i<50:\n",
    "#             continue\n",
    "        \n",
    "        target_sent.append(line)\n",
    "        if len(target_sent)>=sentences_to_read:\n",
    "            break\n",
    "        \n",
    "            \n",
    "assert len(source_sent)==len(target_sent),'Source: %d, Target: %d'%(len(source_sent),len(target_sent))\n",
    "\n",
    "print('Sample translations (%d)'%len(source_sent))\n",
    "for i in range(0,sentences_to_read,10000):\n",
    "    print('(',i,') Table: ', source_sent[i])\n",
    "    print('(',i,') Summary: ', target_sent[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_tokens(sent,is_source):\n",
    "    #sent = sent.replace('-',' ')\n",
    "#     sent = sent.replace(',','')\n",
    "#     sent = sent.replace('.','')\n",
    "    sent = sent.replace('\\n','') \n",
    "    \n",
    "    sent_toks = sent.split(' ')\n",
    "    for t_i, tok in enumerate(sent_toks):\n",
    "        if is_source:\n",
    "            if tok not in src_dictionary.keys():\n",
    "                sent_toks[t_i] = '<unk>'\n",
    "                #print tok\n",
    "        else:\n",
    "            if tok not in tgt_dictionary.keys():\n",
    "                sent_toks[t_i] = '<unk>'\n",
    "                #print(tok)\n",
    "\n",
    "    return sent_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Source) Sentence mean length:  89.21\n",
      "(Source) Sentence stddev length:  14.010963564294926\n",
      "(Source) Sentence max length:  122\n",
      "(Target) Sentence mean length:  30.59104\n",
      "(Target) Sentence stddev length:  13.863538931975485\n",
      "(Target) Sentence max length:  88\n"
     ]
    }
   ],
   "source": [
    "# Let us first look at some statistics of the sentences\n",
    "source_len = []\n",
    "source_mean, source_std = 0,0\n",
    "for sent in source_sent:\n",
    "    source_len.append(len(split_to_tokens(sent.strip(),True)))\n",
    "\n",
    "print('(Source) Sentence mean length: ', np.mean(source_len))\n",
    "print('(Source) Sentence stddev length: ', np.std(source_len))\n",
    "print('(Source) Sentence max length: ', np.max(source_len))\n",
    "#print source_len\n",
    "\n",
    "target_len = []\n",
    "target_mean, target_std = 0,0\n",
    "for sent in target_sent:\n",
    "    target_len.append(len(split_to_tokens(sent.strip(),False)))\n",
    "\n",
    "print('(Target) Sentence mean length: ', np.mean(target_len))\n",
    "print('(Target) Sentence stddev length: ', np.std(target_len))\n",
    "print('(Target) Sentence max length: ', np.max(target_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sent lengths:  0\n",
      "Samples from bin\n",
      "\t ['<s>', 'temperature', 'time', '6-21', 'min', '26', 'mean', '43', 'max', '53', 'windChill', 'time', '6-21', 'min', '0', 'mean', '22', 'max', '46', 'windSpeed', 'time', '6-21', 'min', '6', 'mean', '8', 'max', '10', 'mode-bucket-0-20-2', '0-10', 'windDir', 'time', '6-21', 'mode', 'SE', 'gust', 'time', '6-21', 'min', '0', 'mean', '0', 'max', '0', 'skyCover', 'time', '6-21', 'mode-bucket-0-100-4', '50-75', 'skyCover', 'time', '6-9', 'mode-bucket-0-100-4', '0-25', 'skyCover', 'time', '6-13', 'mode-bucket-0-100-4', '25-50', 'skyCover', 'time', '9-21', 'mode-bucket-0-100-4', '50-75', 'skyCover', 'time', '13-21', 'mode-bucket-0-100-4', '50-75', 'precipPotential', 'time', '6-21', 'min', '0', 'mean', '2', 'max', '12', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "\t ['<s>', 'Mostly', 'sunny', ',', 'with', 'a', 'high', 'near', '53', '.', 'Southeast', 'wind', 'between', '7', 'and', '9', 'mph', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "\t ['<s>', 'temperature', 'time', '17-30', 'min', '59', 'mean', '64', 'max', '75', 'windChill', 'time', '17-30', 'min', '0', 'mean', '0', 'max', '0', 'windSpeed', 'time', '17-30', 'min', '9', 'mean', '10', 'max', '17', 'mode-bucket-0-20-2', '0-10', 'windDir', 'time', '17-30', 'mode', 'SSE', 'gust', 'time', '17-30', 'min', '0', 'mean', '4', 'max', '23', 'skyCover', 'time', '17-30', 'mode-bucket-0-100-4', '50-75', 'skyCover', 'time', '17-21', 'mode-bucket-0-100-4', '50-75', 'skyCover', 'time', '17-26', 'mode-bucket-0-100-4', '50-75', 'skyCover', 'time', '21-30', 'mode-bucket-0-100-4', '50-75', 'skyCover', 'time', '26-30', 'mode-bucket-0-100-4', '50-75', 'precipPotential', 'time', '17-30', 'min', '0', 'mean', '5', 'max', '10', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "\t ['<s>', 'Mostly', 'cloudy', ',', 'with', 'a', 'low', 'around', '59', '.', 'South', 'wind', 'between', '10', 'and', '15', 'mph', ',', 'with', 'gusts', 'as', 'high', 'as', '20', 'mph', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "\n",
      "\tSentences  25000\n",
      "train inp lengths [ 79 104  79 ... 104  79 104]\n",
      "train out lengths [19 23 19 ... 33 27 32]\n"
     ]
    }
   ],
   "source": [
    "train_inputs = []\n",
    "train_outputs = []\n",
    "train_inp_lengths = []\n",
    "train_out_lengths = []\n",
    "\n",
    "max_tgt_sent_lengths = 0\n",
    "\n",
    "src_max_sent_length = source_sequence_length+1\n",
    "tgt_max_sent_length = target_sequence_length+1\n",
    "for s_i, (src_sent, tgt_sent) in enumerate(zip(source_sent,target_sent)):\n",
    "    \n",
    "    src_sent_tokens = split_to_tokens(src_sent.strip(),True)\n",
    "    tgt_sent_tokens = split_to_tokens(tgt_sent.strip(),False)\n",
    "        \n",
    "    num_src_sent = []\n",
    "    for tok in src_sent_tokens:\n",
    "        num_src_sent.append(src_dictionary[tok])\n",
    "\n",
    "    num_src_set = num_src_sent[::-1] # we reverse the source sentence. This improves performance\n",
    "    num_src_sent.insert(0,src_dictionary['<s>'])\n",
    "    train_inp_lengths.append(min(len(num_src_sent)+1,src_max_sent_length))\n",
    "    \n",
    "    # append until the sentence reaches max length\n",
    "    if len(num_src_sent)<src_max_sent_length:\n",
    "        num_src_sent.extend([src_dictionary['</s>'] for _ in range(src_max_sent_length - len(num_src_sent))])\n",
    "    # if more than max length, truncate the sentence\n",
    "    elif len(num_src_sent)>src_max_sent_length:\n",
    "        num_src_sent = num_src_sent[:src_max_sent_length]\n",
    "    assert len(num_src_sent)==src_max_sent_length,len(num_src_sent)\n",
    "\n",
    "    train_inputs.append(num_src_sent)\n",
    "\n",
    "    #num_tgt_sent = [tgt_dictionary['</s>']]\n",
    "    num_tgt_sent = [tgt_dictionary['<s>']]\n",
    "    for tok in tgt_sent_tokens:\n",
    "        num_tgt_sent.append(tgt_dictionary[tok])\n",
    "    \n",
    "    #num_tgt_sent.insert(0,tgt_dictionary['<s>'])\n",
    "    train_out_lengths.append(min(len(num_tgt_sent)+1,tgt_max_sent_length))\n",
    "    \n",
    "    if len(num_tgt_sent)<tgt_max_sent_length:\n",
    "        num_tgt_sent.extend([tgt_dictionary['</s>'] for _ in range(tgt_max_sent_length - len(num_tgt_sent))])\n",
    "    elif len(num_tgt_sent)>tgt_max_sent_length:\n",
    "        num_tgt_sent = num_tgt_sent[:tgt_max_sent_length]\n",
    "    \n",
    "    train_outputs.append(num_tgt_sent)\n",
    "    assert len(train_outputs[s_i])==tgt_max_sent_length, 'Sent length needs to be 60, but is %d'%len(binned_outputs[s_i])    \n",
    "\n",
    "assert len(train_inputs)  == len(source_sent),\\\n",
    "        'Size of total bin elements: %d, Total sentences: %d'\\\n",
    "                %(len(train_inputs),len(source_sent))\n",
    "\n",
    "print('Max sent lengths: ', max_tgt_sent_lengths)\n",
    "\n",
    "\n",
    "train_inputs = np.array(train_inputs, dtype=np.int32)\n",
    "train_outputs = np.array(train_outputs, dtype=np.int32)\n",
    "train_inp_lengths = np.array(train_inp_lengths, dtype=np.int32)\n",
    "train_out_lengths = np.array(train_out_lengths, dtype=np.int32)\n",
    "print('Samples from bin')\n",
    "print('\\t',[src_reverse_dictionary[w]  for w in train_inputs[0,:].tolist()])\n",
    "print('\\t',[tgt_reverse_dictionary[w]  for w in train_outputs[0,:].tolist()])\n",
    "print('\\t',[src_reverse_dictionary[w]  for w in train_inputs[10,:].tolist()])\n",
    "print('\\t',[tgt_reverse_dictionary[w]  for w in train_outputs[10,:].tolist()])\n",
    "print()\n",
    "print('\\tSentences ',train_inputs.shape[0])\n",
    "print('train inp lengths',train_inp_lengths)\n",
    "print('train out lengths',train_out_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 256\n",
    "\n",
    "class DataGeneratorMT(object):\n",
    "    \n",
    "    def __init__(self,batch_size,num_unroll,is_source):\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        \n",
    "        \n",
    "        self._src_word_embeddings = np.load('./data/table_vec.npy')\n",
    "        \n",
    "        self._tgt_word_embeddings = np.load('./data/summary_vec.npy')\n",
    "        \n",
    "        self._sent_ids = None\n",
    "        \n",
    "        self._is_source = is_source\n",
    "        \n",
    "                \n",
    "    def next_batch(self, sent_ids, first_set):\n",
    "        \n",
    "        if self._is_source:\n",
    "            max_sent_length = src_max_sent_length\n",
    "        else:\n",
    "            max_sent_length = tgt_max_sent_length\n",
    "        batch_labels_ind = []\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float64)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float64)\n",
    "        \n",
    "        for b in range(self._batch_size):\n",
    "            \n",
    "            sent_id = sent_ids[b]\n",
    "            \n",
    "            if self._is_source:\n",
    "                sent_text = train_inputs[sent_id]\n",
    "                             \n",
    "                batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b]=sent_text[self._cursor[b]+1]\n",
    "\n",
    "            else:\n",
    "                sent_text = train_outputs[sent_id]\n",
    "                \n",
    "                # We cannot avoid having two different embedding vectors for <s> token\n",
    "                # in soruce and target languages\n",
    "                # Therefore, if the symbol appears, we always take the source embedding vector\n",
    "                if sent_text[self._cursor[b]]!=src_dictionary['<s>']:\n",
    "                    batch_data[b] = sent_text[self._cursor[b]]\n",
    "                else:\n",
    "                    batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b] = sent_text[self._cursor[b]+1]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%(max_sent_length-1)\n",
    "                                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self,sent_ids):\n",
    "        \n",
    "        if sent_ids is not None:\n",
    "            \n",
    "            self._sent_ids = sent_ids\n",
    "            \n",
    "            #if self._is_source:\n",
    "                # we dont star at the very beginning, becaues the very beginning is a bunch of </s> symbols.\n",
    "                # so we start from the middel s.t we get a minimum number of </s> symbols in our training data\n",
    "                # this is only needed for source language\n",
    "                #self._cursor = ((start_indices_for_bins[bin_id][self._sent_ids]//self._num_unroll)*self._num_unroll).tolist()\n",
    "            #else:\n",
    "            self._cursor = [0 for _ in range(self._batch_size)]\n",
    "                \n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        inp_lengths = None\n",
    "        for ui in range(self._num_unroll):\n",
    "            # The first batch in any batch of captions is different\n",
    "            if self._is_source:\n",
    "                data, labels = self.next_batch(self._sent_ids, False)\n",
    "            else:\n",
    "                data, labels = self.next_batch(self._sent_ids, False)\n",
    "                    \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "            if self._is_source:\n",
    "                inp_lengths = train_inp_lengths[sent_ids]\n",
    "            else:\n",
    "                inp_lengths = train_out_lengths[sent_ids]\n",
    "        return unroll_data, unroll_labels, self._sent_ids, inp_lengths\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "enc_train_inputs = []\n",
    "dec_train_inputs = []\n",
    "\n",
    "# Need to use pre-trained word embeddings\n",
    "encoder_emb_layer = tf.convert_to_tensor(np.load('./data/table_vec.npy'))\n",
    "decoder_emb_layer = tf.convert_to_tensor(np.load('./data/summary_vec.npy'))\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(source_sequence_length):\n",
    "    enc_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size],name='enc_train_inputs_%d'%ui))\n",
    "\n",
    "dec_train_labels=[]\n",
    "dec_label_masks = []\n",
    "for ui in range(target_sequence_length):\n",
    "    dec_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size],name='dec_train_inputs_%d'%ui))\n",
    "    dec_train_labels.append(tf.placeholder(tf.int32, shape=[batch_size],name='dec-train_outputs_%d'%ui))\n",
    "    dec_label_masks.append(tf.placeholder(tf.float64, shape=[batch_size],name='dec-label_masks_%d'%ui))\n",
    "    \n",
    "encoder_emb_inp = [tf.nn.embedding_lookup(encoder_emb_layer, src) for src in enc_train_inputs]\n",
    "encoder_emb_inp = tf.stack(encoder_emb_inp)\n",
    "\n",
    "decoder_emb_inp = [tf.nn.embedding_lookup(decoder_emb_layer, src) for src in dec_train_inputs]\n",
    "decoder_emb_inp = tf.stack(decoder_emb_inp)\n",
    "\n",
    "enc_train_inp_lengths = tf.placeholder(tf.int32, shape=[batch_size],name='train_input_lengths')\n",
    "dec_train_inp_lengths = tf.placeholder(tf.int32, shape=[batch_size],name='train_output_lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'train_output_lengths:0' shape=(32,) dtype=int32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train_inp_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float64)\n",
    "\n",
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_emb_inp, initial_state=initial_state,\n",
    "    sequence_length=enc_train_inp_lengths, \n",
    "    time_major=True, swap_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(32, 512) dtype=float64>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(32, 512) dtype=float64>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "# backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "# ((encoder_fw_outputs,\n",
    "#   encoder_bw_outputs),\n",
    "#  (encoder_fw_final_state,\n",
    "#   encoder_bw_final_state)) = tf.nn.bidirectional_dynamic_rnn(\n",
    "#     forward_cell, backward_cell, encoder_emb_inp, dtype = tf.float64,\n",
    "#     sequence_length=enc_train_inp_lengths, time_major=True,swap_memory=True)\n",
    "\n",
    "# encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train_inp_lengths.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/TensorArrayStack/TensorArrayGatherV3:0' shape=(122, 32, 512) dtype=float64>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/TensorArrayStack/TensorArrayGatherV3:0\", shape=(122, 32, 512), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print( encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_final_state_c = tf.concat(\n",
    "#     (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "# encoder_final_state_h = tf.concat(\n",
    "#     (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "# #TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "# encoder_final_state = LSTMStateTuple(\n",
    "#     c=encoder_final_state_c,\n",
    "#     h=encoder_final_state_h\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_state=encoder_final_state\n",
    "# print encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_state[0].get_shape().as_list()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack_1:0' shape=(88, 32, 256) dtype=float64>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_emb_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RNN cell\n",
    "#num_units = encoder_state[0].get_shape().as_list()[1]\n",
    "decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "projection_layer = Dense(units=vocab_size, use_bias=True)\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_emb_inp, [tgt_max_sent_length-1 for _ in range(batch_size)], time_major=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "#     decoder_emb_layer,\n",
    "#     tf.fill([batch_size], tgt_dictionary[tgt_sos_id]), tgt_dictionary[tgt_eos_id])\n",
    "# Decoder\n",
    "if decoder_type == 'basic':\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, encoder_state,\n",
    "        output_layer=projection_layer)\n",
    "    \n",
    "elif decoder_type == 'attention':\n",
    "    decoder = tf.contrib.seq2seq.BahdanauAttention(\n",
    "        decoder_cell, helper, encoder_state,\n",
    "        output_layer=projection_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "   \n",
    "# Dynamic decoding\n",
    "outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, output_time_major=True,\n",
    "    swap_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.rnn_output\n",
    "\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=dec_train_labels, logits=logits)\n",
    "loss = (tf.reduce_sum(crossent*tf.stack(dec_label_masks)) / (batch_size*target_sequence_length))\n",
    "#loss = tf.reduce_mean(crossent)\n",
    "\n",
    "train_prediction = outputs.sample_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Optimizer\n"
     ]
    }
   ],
   "source": [
    "print('Defining Optimizer')\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    0.01, global_step, decay_steps=10, decay_rate=0.9, staircase=True)\n",
    "\n",
    "with tf.variable_scope('Adam'):\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "adam_gradients, v = zip(*adam_optimizer.compute_gradients(loss))\n",
    "adam_gradients, _ = tf.clip_by_global_norm(adam_gradients, 25.0)\n",
    "adam_optimize = adam_optimizer.apply_gradients(zip(adam_gradients, v))\n",
    "\n",
    "with tf.variable_scope('SGD'):\n",
    "    sgd_optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "sgd_gradients, v = zip(*sgd_optimizer.compute_gradients(loss))\n",
    "sgd_gradients, _ = tf.clip_by_global_norm(sgd_gradients, 25.0)\n",
    "sgd_optimize = sgd_optimizer.apply_gradients(zip(sgd_gradients, v))\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'train_input_lengths:0' shape=(32,) dtype=int32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train_inp_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('logs'):\n",
    "    os.mkdir('logs')\n",
    "log_dir = 'logs'\n",
    "\n",
    "bleu_scores_over_time = []\n",
    "loss_over_time = []\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "src_word_embeddings = np.load('./data/table_vec.npy')\n",
    "tgt_word_embeddings = np.load('./data/summary_vec.npy')\n",
    "\n",
    "# Defining data generators\n",
    "enc_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=source_sequence_length,is_source=True)\n",
    "dec_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=target_sequence_length,is_source=False)\n",
    "\n",
    "num_steps = 2\n",
    "avg_loss = 0\n",
    "\n",
    "bleu_labels, bleu_preds = [],[]\n",
    "\n",
    "print('Started Training')\n",
    "\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # input_sizes for each bin: [40]\n",
    "    # output_sizes for each bin: [60]\n",
    "    #print ('.'),\n",
    "    if (step+1)%100==0:\n",
    "        print('')\n",
    "        \n",
    "    sent_ids = np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
    "    # ====================== ENCODER ================================================\n",
    "    \n",
    "    eu_data, eu_labels, _, eu_lengths = enc_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "    \n",
    "    feed_dict = {}\n",
    "    feed_dict[enc_train_inp_lengths] = eu_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(eu_data,eu_labels)):            \n",
    "        feed_dict[enc_train_inputs[ui]] = dat                \n",
    "    \n",
    "    # ====================== DECODER ===========================\n",
    "    # First step we change the ids in a batch\n",
    "    du_data, du_labels, _, du_lengths = dec_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "    \n",
    "    feed_dict[dec_train_inp_lengths] = du_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(du_data,du_labels)):            \n",
    "        feed_dict[dec_train_inputs[ui]] = dat\n",
    "        feed_dict[dec_train_labels[ui]] = lbl\n",
    "        feed_dict[dec_label_masks[ui]] = (np.array([ui for _ in range(batch_size)])<du_lengths).astype(np.int32)\n",
    "    \n",
    "    # ======================= OPTIMIZATION ==========================\n",
    "    if step < 10000:\n",
    "        _,l,tr_pred = sess.run([adam_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "    else:\n",
    "        _,l,tr_pred = sess.run([sgd_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "    tr_pred = tr_pred.flatten()\n",
    "        \n",
    "            \n",
    "    if (step+1)%250==0:  \n",
    "        \n",
    "        print('Step ',step+1)\n",
    "\n",
    "        print_str = 'Actual: '\n",
    "        for w in np.concatenate(du_labels,axis=0)[::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '                    \n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "                      \n",
    "        print(print_str)\n",
    "        print()\n",
    "        \n",
    "        print_str = 'Predicted: '\n",
    "        for w in tr_pred[::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "       \n",
    "        print('\\n')  \n",
    "        \n",
    "        rand_idx = np.random.randint(low=1,high=batch_size)\n",
    "        print_str = 'Actual: '\n",
    "        for w in np.concatenate(du_labels,axis=0)[rand_idx::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "\n",
    "            \n",
    "        print()\n",
    "        print_str = 'Predicted: '\n",
    "        for w in tr_pred[rand_idx::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "        print()        \n",
    "        \n",
    "    avg_loss += l\n",
    "    \n",
    "    #sess.run(reset_train_state) # resetting hidden state for each batch\n",
    "    \n",
    "    if (step+1)%500==0:\n",
    "        print('============= Step ', str(step+1), ' =============')\n",
    "        print('\\t Loss: ',avg_loss/500.0)\n",
    "        \n",
    "        loss_over_time.append(avg_loss/500.0)\n",
    "             \n",
    "        avg_loss = 0.0\n",
    "        sess.run(inc_gstep)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feed_dict.values()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
